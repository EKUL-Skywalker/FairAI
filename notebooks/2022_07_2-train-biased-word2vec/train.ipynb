{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook \n",
    "\n",
    "This notebook will demonstrate how to train fairness-aware word2vec based on a triplet contrastive learning. In the triplet contrastive learning, one samples a center word $i$ and a corresponding \"context\" word $j$ that appears around $i$. Then, one samples a \"fake\" context $j'$ from a random distribution P. word2vec is trained to discriminate the authentic center-context pair ($i$,$j$) and a fake center-context pair ($i$,$j'$). word2vec learns an embedding based on data features that best discriminate the authentic and fake pairs. \n",
    "\n",
    "At the heart of the idea is to prevent word2vec to pick features pertained to social biases by generating the fake context $j'$ using a biased model. By generating the fake pair using a biased model, features pertained to social biases become non-informative in the discrimination task, so that the (new) word2vec model will learn an embedding from unbiased component of the given data. \n",
    "\n",
    "To generate a fake context pair $j'$ from a biased word2vec model, remind that the word2vec constructs an embedding by learning a conditional probability \n",
    "$$\n",
    "P(j|i) = \\frac{\\exp(u_i ^ \\top v_j)}{Z},\n",
    "$$\n",
    "where $u_i$ and $v_j$ are the embedding vectors representing center word $i$ and context word $j$, and $Z$ is the normalization constant. Using the conditional probability, we can sample a fake context $j'$ from the conditional probability $P(j'|i)$ learned in the biased model. Specifically, given a biased embedding, we generate the fake pair $j'$ from \n",
    "$$\n",
    "P(j'|i) = \\frac{\\exp(\\alpha \\cdot u_i ^ \\top v_{j'})}{Z},\n",
    "$$\n",
    "where $\\alpha$ is the concentration parameter. $\\alpha=0$ yields a uniform distribution, and $\\alpha \\rightarrow \\infty$ leads to a delta function peaked at the maximum similarity. We set $\\alpha=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from scipy import sparse \n",
    "from pathlib import Path\n",
    "import gravlearn # pip install gravlearn\n",
    "import gensim \n",
    "import pickle\n",
    "from tqdm.auto import  tqdm\n",
    "import faiss \n",
    "from numba import njit\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "from utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = Path(\"../../data\")\n",
    "biased_model_file = data_dir / \"derived/simplewiki/models/biased_word2vec.bin\"\n",
    "biased_dataset_id_file = data_dir / \"derived/simplewiki/biased-dataset/dataset.pkl\"\n",
    "dataset_file = data_dir / \"raw/simplewiki/simplewiki-20171103-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "# Output\n",
    "output_file = data_dir / \"derived/simplewiki/models/fairness-aware-word2vec/fairness-aware-word2vec_dim~25.pth\"\n",
    "output_kv_file = data_dir / \"derived/simplewiki/models/fairness-aware-word2vec/fairness-aware-word2vec-keyedvector_dim~25.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load biased model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biased_model = gensim.models.Word2Vec.load(str(biased_model_file))\n",
    "\n",
    "with open(biased_dataset_id_file, \"rb\") as f: \n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "documents = Dataset(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = biased_model.wv.key_to_index.copy()\n",
    "indexed_documents = [ list(filter(lambda x : x!=-1, map(lambda x : word2index.get(x, -1) , doc ))) for doc in tqdm(documents.lines)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the biased embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(biased_model.wv)\n",
    "dim = biased_model.vector_size\n",
    "in_vec = np.zeros((num_nodes, dim))\n",
    "out_vec = np.zeros((num_nodes, dim))\n",
    "for i, k in enumerate(biased_model.wv.index_to_key):\n",
    "    in_vec[i, :] = biased_model.wv[k]\n",
    "    out_vec[i, :] = biased_model.syn1neg[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a dataset for triplet contrastive learning. Our dataset consists of two samplers, one for anchor-positive example pairs, and the other for anchor-negative example pairs.\n",
    "First, let us define the sampler for anchor-positive pairs. We will use nGramSampler, which samples the pairs from a given word sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sampler = gravlearn.nGramSampler(\n",
    "    window_length=10, context_window_type=\"double\", buffer_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a sampler for anchor-negative pairs based on a soft-max function \n",
    "\n",
    "$$P(j'|i) = \\exp(u_i ^\\top v_j') / Z$$\n",
    "\n",
    "\n",
    "where u_i and v_j are the in-vector and out-vector representing center word i and context word j', respectively. \n",
    "However, evaluating the probability is computationally expensive due to the normalization constant Z that extends over all nodes in the dataset. \n",
    "\n",
    "To reduce the burden, I'll use an alternative based on the two-stage sampling as follows. \n",
    "1. First, I find the top $m=500$ words with the largest $\\exp(u_i ^\\top v_j')$ for each center word $i$. \n",
    "2. With probability $\\alpha$, we draw context $j'$ from the $m=500$ closest words with probability proportional to $\\exp(u_i ^\\top v_j')$. \n",
    "3. Otherwise, we draw j' from all nodes with probability proportional to the frequency. \n",
    "\n",
    "Here, alpha is a hyper-parameter that controls the balance between contextual and non-contextual sampling. I set $\\alpha=0.9$ for this experiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSampler(gravlearn.DataSampler):\n",
    "    def __init__(self, in_vec, out_vec, alpha=0.9, m = 500, gpu_id = None):\n",
    "        self.alpha = alpha\n",
    "        self.in_vec = in_vec.astype(\"float32\")\n",
    "        self.out_vec = out_vec.astype(\"float32\")\n",
    "        self.center_sampler = gravlearn.FrequencyBasedSampler()\n",
    "        self.n_elements, self.dim = out_vec.shape[0], self.out_vec.shape[1]\n",
    "        \n",
    "        #\n",
    "        # Find the m words with the largest probability mass\n",
    "        #\n",
    "\n",
    "        # Make faiss index\n",
    "        n_train_sample = np.minimum(100000, self.n_elements)\n",
    "        nlist = int(np.ceil(np.sqrt(n_train_sample)))\n",
    "        faiss_index = faiss.IndexIVFFlat(faiss.IndexFlatIP(self.dim), self.dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        if gpu_id is not None:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            faiss_index = faiss.index_cpu_to_gpu(res, gpu_id, faiss_index)\n",
    "        faiss_index.train(self.out_vec[np.random.choice(self.n_elements, n_train_sample, replace=False)])\n",
    "\n",
    "        # Add the embedding vectors to index \n",
    "        faiss_index.add(self.out_vec)\n",
    "\n",
    "        # Construct a graph of words with edges running between a center word $i$ and the m nodes with the largest $\\exp(u_i ^\\top v_j)$.  \n",
    "        dist, indices = faiss_index.search(self.in_vec, m)\n",
    "        rows = np.arange(self.n_elements).reshape((-1, 1)) @ np.ones((1, m))\n",
    "        rows, indices, dist = rows.ravel(), indices.ravel(), dist.ravel()\n",
    "        s = indices >= 0\n",
    "        rows, indices, dist = rows[s], indices[s], dist[s]\n",
    "        dist = np.exp(dist)\n",
    "        A = sparse.csr_matrix(\n",
    "            (dist, (rows, indices)),\n",
    "            shape=(self.n_elements, self.n_elements),\n",
    "        )\n",
    "\n",
    "        # Preprocess the graph for faster sampling \n",
    "        data = A.data / A.sum(axis=1).A1.repeat(np.diff(A.indptr))\n",
    "        A.data = _csr_row_cumsum(A.indptr, data)\n",
    "\n",
    "        self.A = A\n",
    "\n",
    "    def fit(self, seqs):\n",
    "        self.center_sampler.fit(seqs)\n",
    "\n",
    "    def conditional_sampling(self, conditioned_on=None):\n",
    "        if np.random.rand() < self.alpha:\n",
    "            return _sample_one_neighbor(conditioned_on, self.A.indptr, self.A.indices, self.A.data)\n",
    "        else:\n",
    "            return self.center_sampler.sampling()[0]\n",
    "\n",
    "    def sampling(self):\n",
    "        cent = self.center_sampler.sampling()[0]\n",
    "        cont = self.conditional_sampling(cent)\n",
    "        return cent, cont\n",
    "\n",
    "#\n",
    "# Helper functions\n",
    "#\n",
    "@njit(nogil=True)\n",
    "def _csr_row_cumsum(indptr, data):\n",
    "    out = np.empty_like(data)\n",
    "    for i in range(len(indptr) - 1):\n",
    "        acc = 0\n",
    "        for j in range(indptr[i], indptr[i + 1]):\n",
    "            acc += data[j]\n",
    "            out[j] = acc\n",
    "        out[j] = 1.0\n",
    "    return out\n",
    "\n",
    "@njit(nogil=True)\n",
    "def _sample_one_neighbor(node_id, indptr, indices, data):\n",
    "    neighbors = indices[indptr[node_id]:indptr[node_id + 1]]\n",
    "    neighbors_weight = data[indptr[node_id]:indptr[node_id + 1]]\n",
    "    return neighbors[\n",
    "        np.searchsorted(neighbors_weight, np.random.rand())\n",
    "    ]\n",
    "\n",
    "neg_sampler = Word2VecSampler(in_vec=in_vec, out_vec=out_vec, alpha=0.9, m = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the samplers and bundle them as a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sampler.fit(indexed_documents)\n",
    "pos_sampler.fit(indexed_documents)\n",
    "\n",
    "# Bundle them as a dataset\n",
    "dataset = gravlearn.TripletDataset(\n",
    "    epochs=1, pos_sampler=pos_sampler, neg_sampler=neg_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a word2vec model with node similarity being dot similarity. We will take advantage of a GPU to boost the training process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "dist_metric = gravlearn.metrics.DistanceMetrics.DOTSIM\n",
    "batch_size = 20000\n",
    "checkpoint = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the word2vec model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gravlearn.Word2Vec(vocab_size=num_nodes, dim=dim)\n",
    "model.train()\n",
    "model = model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "dataloader = gravlearn.DataLoader(\n",
    "     dataset,\n",
    "     batch_size=batch_size,\n",
    "     shuffle=False,\n",
    "     num_workers=4,\n",
    "     pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the loss function\n",
    "loss_func = gravlearn.TripletLoss(embedding=model, dist_metric=dist_metric)\n",
    "\n",
    "# The optimizer \n",
    "focal_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optim = torch.optim.AdamW(focal_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(enumerate(dataloader), miniters=100, total=len(dataloader))\n",
    "for it, (p1, p2, n1) in pbar:\n",
    "\n",
    "    # clear out the gradient\n",
    "    focal_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    for param in focal_params:\n",
    "        param.grad = None\n",
    "\n",
    "    # Convert to bags if bags are given\n",
    "    p1, p2, n1 = p1.to(device), p2.to(device), n1.to(device)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(p1, p2, n1)\n",
    "\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(focal_params, 1)\n",
    "\n",
    "    # update the parameters\n",
    "    optim.step()\n",
    "\n",
    "    pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    if (it + 1) % checkpoint == 0:\n",
    "        if output_file is not None:\n",
    "            torch.save(model.state_dict(), output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as the keyed vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vec = model.ivectors.weight.detach().cpu().numpy()\n",
    "kv = gensim.models.KeyedVectors(in_vec.shape[1])\n",
    "kv.add_vectors(biased_model.wv.index_to_key, in_vec)\n",
    "kv.save(\"fairness-word2vec-keyedvector.kv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('authordynamics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ba3cc8ac23911f9837f125f410aa79985736e9a53ede8675efb0dd78c13842c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

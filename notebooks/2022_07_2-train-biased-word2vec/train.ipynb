{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook \n",
    "\n",
    "This notebook will demonstrate how to train fairness-aware word2vec based on a triplet contrastive learning. In the triplet contrastive learning, one samples a center word $i$ and a corresponding \"context\" word $j$ that appears around $i$. Then, one samples a \"fake\" context $j'$ from a random distribution P. word2vec is trained to discriminate the authentic center-context pair ($i$,$j$) and a fake center-context pair ($i$,$j'$). word2vec learns an embedding based on data features that best discriminate the authentic and fake pairs. \n",
    "\n",
    "At the heart of the idea is to prevent word2vec to pick features pertained to social biases by generating the fake context $j'$ using a biased model. By generating the fake pair using a biased model, features pertained to social biases become non-informative in the discrimination task, so that the (new) word2vec model will learn an embedding from unbiased component of the given data. \n",
    "\n",
    "To generate a fake context pair $j'$ from a biased word2vec model, remind that the word2vec constructs an embedding by learning a conditional probability \n",
    "$$\n",
    "P(j|i) = \\frac{\\exp(u_i ^ \\top v_j)}{Z},\n",
    "$$\n",
    "where $u_i$ and $v_j$ are the embedding vectors representing center word $i$ and context word $j$, and $Z$ is the normalization constant. Using the conditional probability, we can sample a fake context $j'$ from the conditional probability $P(j'|i)$ learned in the biased model. Specifically, given a biased embedding, we generate the fake pair $j'$ from \n",
    "$$\n",
    "P(j'|i) = \\frac{\\exp(\\alpha \\cdot u_i ^ \\top v_{j'})}{Z},\n",
    "$$\n",
    "where $\\alpha$ is the concentration parameter. $\\alpha=0$ yields a uniform distribution, and $\\alpha \\rightarrow \\infty$ leads to a delta function peaked at the maximum similarity. We set $\\alpha=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from scipy import sparse \n",
    "from pathlib import Path\n",
    "import gravlearn # pip install gravlearn\n",
    "import gensim \n",
    "import pickle\n",
    "from tqdm.auto import  tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "from utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = Path(\"../../data\")\n",
    "biased_model_file = data_dir / \"derived/simplewiki/models/biased_word2vec.bin\"\n",
    "biased_dataset_id_file = data_dir / \"derived/simplewiki/biased-dataset/dataset.pkl\"\n",
    "dataset_file = data_dir / \"raw/simplewiki/simplewiki-20171103-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "# Output\n",
    "output_file = \"fairness-aware-word2vec.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load biased model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biased_model = gensim.models.Word2Vec.load(str(biased_model_file))\n",
    "\n",
    "with open(biased_dataset_id_file, \"rb\") as f: \n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "documents = Dataset(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = biased_model.wv.key_to_index.copy()\n",
    "indexed_documents = [ list(filter(lambda x : x!=-1, map(lambda x : word2index.get(x, -1) , doc ))) for doc in tqdm(documents.lines)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the biased embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(biased_model.wv)\n",
    "dim = biased_model.vector_size\n",
    "in_vec = np.zeros((num_nodes, dim))\n",
    "out_vec = np.zeros((num_nodes, dim))\n",
    "for i, k in enumerate(biased_model.wv.index_to_key):\n",
    "    in_vec[i, :] = biased_model.wv[k]\n",
    "    out_vec[i, :] = biased_model.syn1neg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nGramSampler samples pairs of center and context word from \n",
    "# a given sequence of words\n",
    "pos_sampler = gravlearn.nGramSampler(\n",
    "    window_length=10, context_window_type=\"double\", buffer_size=1000,\n",
    ")\n",
    "\n",
    "# Word2VecSampler will produce the negative context j' given a center word i based on a soft-max function \n",
    "# \n",
    "# P(j'|i) = exp(alpha u_i ^\\top v_j') / Z\n",
    "#\n",
    "# where u_i and v_j are the in-vector and out-vector representing center word i and context word j', respectively. \n",
    "neg_sampler = gravlearn.Word2VecSampler(in_vec=in_vec, out_vec=out_vec, alpha=1)\n",
    "\n",
    "\n",
    "# Fit the samplers\n",
    "pos_sampler.fit(indexed_documents)\n",
    "neg_sampler.fit(indexed_documents)\n",
    "\n",
    "# Bundle them as a dataset\n",
    "dataset = gravlearn.TripletDataset(\n",
    "    epochs=1, pos_sampler=pos_sampler, neg_sampler=neg_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a word2vec model with node similarity being dot similarity. We will take advantage of a GPU to boost the training process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "dist_metric = gravlearn.metrics.DistanceMetrics.DOTSIM\n",
    "batch_size = 20000\n",
    "checkpoint = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the word2vec model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gravlearn.Word2Vec(vocab_size=num_nodes, dim=dim)\n",
    "model.train()\n",
    "model = model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "dataloader = gravlearn.DataLoader(\n",
    "     dataset,\n",
    "     batch_size=batch_size,\n",
    "     shuffle=False,\n",
    "     num_workers=16,\n",
    "     pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set up the loss function\n",
    "loss_func = gravlearn.TripletLoss(embedding=model, dist_metric=dist_metric)\n",
    "\n",
    "# The optimizer \n",
    "focal_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optim = torch.optim.AdamW(focal_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(enumerate(dataloader), miniters=100, total=len(dataloader))\n",
    "for it, (p1, p2, n1) in pbar:\n",
    "\n",
    "    # clear out the gradient\n",
    "    focal_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    for param in focal_params:\n",
    "        param.grad = None\n",
    "\n",
    "    # Convert to bags if bags are given\n",
    "    p1, p2, n1 = p1.to(device), p2.to(device), n1.to(device)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(p1, p2, n1)\n",
    "\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(focal_params, 1)\n",
    "\n",
    "    # update the parameters\n",
    "    optim.step()\n",
    "\n",
    "    pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    if (it + 1) % checkpoint == 0:\n",
    "        if output_file is not None:\n",
    "            torch.save(model.state_dict(), output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('authordynamics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ba3cc8ac23911f9837f125f410aa79985736e9a53ede8675efb0dd78c13842c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
